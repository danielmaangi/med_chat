{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2 pandas tqdm openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import concurrent\n",
    "import PyPDF2\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "\n",
    "## export OPENAI_API_KEY='XXXX'\n",
    "## echo $OPENAI_API_KEY\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "dir_pdfs = 'data' # have those PDFs stored locally here\n",
    "pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_single_pdf(file_path: str, vector_store_id: str):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n",
    "        attach_response = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        return {\"file\": file_name, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "def upload_pdf_files_to_vector_store(vector_store_id: str):\n",
    "    pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n",
    "    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n",
    "    \n",
    "    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n",
    "            result = future.result()\n",
    "            if result[\"status\"] == \"success\":\n",
    "                stats[\"successful_uploads\"] += 1\n",
    "            else:\n",
    "                stats[\"failed_uploads\"] += 1\n",
    "                stats[\"errors\"].append(result)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def create_vector_store(store_name: str) -> dict:\n",
    "    try:\n",
    "        vector_store = client.vector_stores.create(name=store_name)\n",
    "        details = {\n",
    "            \"id\": vector_store.id,\n",
    "            \"name\": vector_store.name,\n",
    "            \"created_at\": vector_store.created_at,\n",
    "            \"file_count\": vector_store.file_counts.completed\n",
    "        }\n",
    "        print(\"Vector store created:\", details)\n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created: {'id': 'vs_67f8b58901c48191984624dbc7ee9dd0', 'name': 'openai_blog_store', 'created_at': 1744352649, 'file_count': 0}\n",
      "1 PDF files to process. Uploading in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:43<00:00, 43.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_files': 1, 'successful_uploads': 1, 'failed_uploads': 0, 'errors': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_name = \"openai_blog_store\"\n",
    "vector_store_details = create_vector_store(store_name)\n",
    "upload_pdf_files_to_vector_store(vector_store_details[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's is the three test algorithm?\"\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store_details['id'],\n",
    "    query=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3243 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.7112310534384182\n",
      "2734 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.5440879896364054\n",
      "3135 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.46890893488627894\n",
      "3237 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.4614953523608751\n",
      "3418 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.4483077232768376\n",
      "3027 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.41732623324363355\n",
      "3026 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.36586905328085234\n",
      "3266 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.32908945274307766\n",
      "3533 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.30706690531996234\n",
      "2906 of character of content from Kenya HIV Prevention and Treatment Guidelines, 2022.pdf with a relevant score of 0.3001532955367745\n"
     ]
    }
   ],
   "source": [
    "for result in search_results.data:\n",
    "    print(str(len(result.content[0].text)) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str(result.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files used: {'Kenya HIV Prevention and Treatment Guidelines, 2022.pdf'}\n",
      "Response:\n",
      "Managing a pregnant woman who has been diagnosed with HIV, elevated liver markers, and tuberculosis (TB) involves a multidisciplinary approach that ensures the health of both the mother and the baby. Here are the recommended management steps based on current guidelines:\n",
      "\n",
      "### 1. **Initiate Antiretroviral Therapy (ART)**\n",
      "- **Start ART**: Initiate lifelong ART for all pregnant women living with HIV, irrespective of their CD4 count or any clinical stage. The preferred first-line regimen is **TDF + 3TC + DTG** (Tenofovir + Lamivudine + Dolutegravir).\n",
      "- **Timing**: Ideally, initiate ART on the same day as the HIV diagnosis.\n",
      "\n",
      "### 2. **Monitor Liver Function**\n",
      "- Given the elevated liver markers, closer monitoring of liver function tests (e.g., ALT) is critical.\n",
      "- Liver function should be assessed regularly as the co-infection with TB and potential hepatotoxicity from ART necessitates vigilance.\n",
      "\n",
      "### 3. **Manage Tuberculosis (TB)**\n",
      "- **Immediate Treatment**: Start anti-TB treatment as soon as TB is diagnosed. The regimen should include first-line TB medications.\n",
      "- **ART and TB Co-infection**: Begin ART ideally within 2 weeks of starting TB therapy, except in cases of TB meningitis, where ART should be deferred for 4-8 weeks depending on stability.\n",
      "\n",
      "### 4. **Adherence and Monitoring**\n",
      "- **Viral Load Monitoring**: Obtain viral load (VL) testing 3 months after initiating ART and continue monitoring every 6 months.\n",
      "- **Support**: Provide enhanced adherence support to ensure that the mother can maintain viral suppression.\n",
      "\n",
      "### 5. **Screen for Other Infections**\n",
      "- Implement regular screenings for syphilis and hepatitis B co-infection, given the elevated risk presented by her diagnoses.\n",
      "\n",
      "### 6. **Nutrition and General Health**\n",
      "- Advise on proper nutrition during pregnancy to support maternal and fetal health. This includes iron and folate supplementation, as well as monitoring for anemia.\n",
      "\n",
      "### 7. **Infant Prophylaxis**\n",
      "- Infants born to mothers with HIV should receive proper prophylaxis (e.g., AZT + NVP) and be monitored for HIV exposure.\n",
      "\n",
      "### 8. **Address Mental Health**\n",
      "- Consider counseling and mental health support for the mother due to the psychological impact of these health conditions.\n",
      "\n",
      "### Conclusion\n",
      "This approach ensures the well-being of both the mother and the child while managing the complexities of HIV and TB co-infection in a pregnant woman. Regular follow-ups and multidisciplinary care are vital to adapt the management plan as the pregnancy progresses.\n"
     ]
    }
   ],
   "source": [
    "query = \"My client has just been diagnosed with HIV. She is pregnant, has elevated liver markers and has TB. How should I manage her?\"\n",
    "response = client.responses.create(\n",
    "    input= query,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store_details['id']],\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Extract annotations from the response\n",
    "annotations = response.output[1].content[0].annotations\n",
    "    \n",
    "# Get top-k retrieved filenames\n",
    "retrieved_files = set([result.filename for result in annotations])\n",
    "\n",
    "print(f'Files used: {retrieved_files}')\n",
    "print('Response:')\n",
    "print(response.output[1].content[0].text) # 0 being the filesearch call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def generate_multiple_questions(pdf_path, num_questions=3, tokens_per_chunk=6000):\n",
    "    \"\"\"Generate multiple questions from different parts of a PDF\"\"\"\n",
    "    questions = []\n",
    "    \n",
    "    # Extract text from different sections of the PDF\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return questions\n",
    "    \n",
    "    # Divide document into sections\n",
    "    text_length = len(full_text)\n",
    "    chars_per_section = text_length // num_questions\n",
    "    \n",
    "    # Generate a question from each section\n",
    "    for i in range(num_questions):\n",
    "        start = i * chars_per_section\n",
    "        end = min(start + (tokens_per_chunk * 4), text_length)\n",
    "        section_text = full_text[start:end]\n",
    "        \n",
    "        prompt = (\n",
    "            f\"This is section {i+1} of {num_questions} from a document. \"\n",
    "            \"Can you generate a question that can only be answered from this section?:\\n\"\n",
    "            f\"{section_text}\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                input=prompt,\n",
    "                model=\"gpt-4o\",\n",
    "            )\n",
    "            question = response.output[0].content[0].text\n",
    "            questions.append(question)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating question for section {i+1}: {e}\")\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is the main theme of the 2022 edition of the Kenya HIV Prevention and Treatment Guidelines as stated in the document's foreword?\",\n",
       " 'What is the recommended starting dose of atorvastatin for patients not on a PI/r, and how long should you wait before repeating fasting lipids to titrate the dose?',\n",
       " 'What diagnostic steps should be followed for a patient who tests positive on smear microscopy but has a negative GeneXpert MTB result?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_multiple_questions(pdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions for each PDF and store in a dictionary\n",
    "questions_dict = {}\n",
    "for pdf_path in pdf_files:\n",
    "    questions = generate_multiple_questions(pdf_path)\n",
    "    questions_dict[os.path.basename(pdf_path)] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Kenya HIV Prevention and Treatment Guidelines, 2022.pdf': ['What is the theme of the Kenya HIV Prevention and Treatment Guidelines, 2022, according to the foreword?',\n",
       "  'What is the advised approach if a patient with diabetes does not meet treatment targets with metformin for 3-6 months at the maximum tolerated dose?',\n",
       "  'What are the considerations and actions involved in managing a presumptive TB case when both the GeneXpert and smear microscopy are unavailable on site?']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for filename, query in questions_dict.items():\n",
    "    rows.append({\"query\": query, \"_id\": filename.replace(\".pdf\", \"\")})\n",
    "\n",
    "# Metrics evaluation parameters\n",
    "k = 5\n",
    "total_queries = len(rows)\n",
    "correct_retrievals_at_k = 0\n",
    "reciprocal_ranks = []\n",
    "average_precisions = []\n",
    "\n",
    "def process_query(row):\n",
    "    query_list = row['query']  # This is a list of strings\n",
    "    if isinstance(query_list, list):\n",
    "        query = query_list[0]  # Let's use the first question for testing\n",
    "    else:\n",
    "        query = query_list  # In case it's already a string\n",
    "        \n",
    "    expected_filename = row['_id'] + '.pdf'\n",
    "    \n",
    "    # Call file_search via Responses API\n",
    "    response = client.responses.create(\n",
    "        input=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\"type\": \"input_text\", \"text\": query}]  # Now query is a string\n",
    "        }],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_details['id']],\n",
    "            \"max_num_results\": k,\n",
    "        }],\n",
    "        tool_choice=\"required\"\n",
    "    )\n",
    "    # Extract annotations from the response\n",
    "    annotations = None\n",
    "    if hasattr(response.output[1], 'content') and response.output[1].content:\n",
    "        annotations = response.output[1].content[0].annotations\n",
    "    elif hasattr(response.output[1], 'annotations'):\n",
    "        annotations = response.output[1].annotations\n",
    "\n",
    "    if annotations is None:\n",
    "        print(f\"No annotations for query: {query}\")\n",
    "        return False, 0, 0\n",
    "\n",
    "    # Get top-k retrieved filenames\n",
    "    retrieved_files = [result.filename for result in annotations[:k]]\n",
    "    if expected_filename in retrieved_files:\n",
    "        rank = retrieved_files.index(expected_filename) + 1\n",
    "        rr = 1 / rank\n",
    "        correct = True\n",
    "    else:\n",
    "        rr = 0\n",
    "        correct = False\n",
    "\n",
    "    # Calculate Average Precision\n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    for i, fname in enumerate(retrieved_files):\n",
    "        if fname == expected_filename:\n",
    "            num_relevant += 1\n",
    "            precisions.append(num_relevant / (i + 1))\n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    \n",
    "    if expected_filename not in retrieved_files:\n",
    "        print(\"Expected file NOT found in the retrieved files!\")\n",
    "        \n",
    "    if retrieved_files and retrieved_files[0] != expected_filename:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Expected file: {expected_filename}\")\n",
    "        print(f\"First retrieved file: {retrieved_files[0]}\")\n",
    "        print(f\"Retrieved files: {retrieved_files}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    \n",
    "    return correct, rr, avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 1.0, 1.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query(rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.59s/it]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_query, rows), total=total_queries))\n",
    "\n",
    "correct_retrievals_at_k = 0\n",
    "reciprocal_ranks = []\n",
    "average_precisions = []\n",
    "\n",
    "for correct, rr, avg_precision in results:\n",
    "    if correct:\n",
    "        correct_retrievals_at_k += 1\n",
    "    reciprocal_ranks.append(rr)\n",
    "    average_precisions.append(avg_precision)\n",
    "\n",
    "recall_at_k = correct_retrievals_at_k / total_queries\n",
    "precision_at_k = recall_at_k  # In this context, same as recall\n",
    "mrr = sum(reciprocal_ranks) / total_queries\n",
    "map_score = sum(average_precisions) / total_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at k=5:\n",
      "Recall@5: 1.0000\n",
      "Precision@5: 1.0000\n",
      "Mean Reciprocal Rank (MRR): 1.0000\n",
      "Mean Average Precision (MAP): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics with k\n",
    "print(f\"Metrics at k={k}:\")\n",
    "print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
    "print(f\"Precision@{k}: {precision_at_k:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
